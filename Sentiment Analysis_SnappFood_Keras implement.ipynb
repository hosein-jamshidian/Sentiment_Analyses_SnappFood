{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hosein-jamshidian/Sentiment_Analyses_SnappFood/blob/main/Sentiment%20Analysis_SnappFood_Keras%20implement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTQYcWop00zM"
      },
      "source": [
        "## libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYj8wUfYavbn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8DKZqO804IX"
      },
      "source": [
        "## read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s_MaDhDbFuX"
      },
      "outputs": [],
      "source": [
        "!unzip /content/DL-HW3.zip\n",
        "!rm /content/DL-HW3.zip\n",
        "!rm /content/DL-HW3/DL-HW3-Description.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('/content/DL-HW3/Snappfood-Dataset/train.csv',sep='\\t',index_col=0)\n",
        "val_df=pd.read_csv('/content/DL-HW3/Snappfood-Dataset/dev.csv',sep='\\t',index_col=0)\n",
        "test_df=pd.read_csv('/content/DL-HW3/Snappfood-Dataset/test.csv',sep='\\t',index_col=0)"
      ],
      "metadata": {
        "id": "uSNN3rrjnp4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHG9c3d8eFrB"
      },
      "outputs": [],
      "source": [
        "train_df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "o0PLh1GsoQ2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG-siLl8byZP"
      },
      "outputs": [],
      "source": [
        "train_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWZ89rBgEwHz"
      },
      "source": [
        "## visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm\n",
        "import hazm"
      ],
      "metadata": {
        "id": "32Q_ptQGpGhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['sent_len']=train_df['comment'].apply(lambda x : len(hazm.word_tokenize(x)))"
      ],
      "metadata": {
        "id": "MkApfaejqoGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(train_df['sent_len'], bins=100)\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxenplot(x=\"label\",y=\"sent_len\",data=train_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "frliz7kPqPOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMMENT_MAX_LEN=50\n",
        "train_df['sent_len']=train_df['comment'].apply(lambda x : len(hazm.word_tokenize(x)[:COMMENT_MAX_LEN]))"
      ],
      "metadata": {
        "id": "6AqSSXD3q0Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(train_df['sent_len'], bins=100)\n",
        "plt.subplot(1,2,2)\n",
        "sns.boxenplot(x=\"label\",y=\"sent_len\",data=train_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwvOHuQHokDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### remove comment with less than 3 words"
      ],
      "metadata": {
        "id": "oz-tydG3vfN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=train_df.drop(train_df[train_df['sent_len'] <= 3].index,axis=0).reset_index(drop=True)\n",
        "\n",
        "\n",
        "val_df['sent_len']=val_df['comment'].apply(lambda x : len(hazm.word_tokenize(x)))\n",
        "val_df=val_df.drop(val_df[val_df['sent_len'] <= 3].index,axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "qAYhjRBltDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RkMbu0lE429"
      },
      "source": [
        "## preprocessing and clean comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDRbS7dUdLCS"
      },
      "outputs": [],
      "source": [
        "!pip install finglish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92bPPC-dLEg"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import hazm\n",
        "from finglish import f2p\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxXIykXwdLG4"
      },
      "outputs": [],
      "source": [
        "hazm_normalizer=hazm.Normalizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7W4Hp2Ye3qu"
      },
      "outputs": [],
      "source": [
        "def cleaning(sent):\n",
        "  wierd_pattern=re.compile(\"[\"\n",
        "      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "      u\"\\U00002702-\\U000027B0\"\n",
        "      u\"\\U000024C2-\\U0001F251\"\n",
        "      u\"\\U0001f926-\\U0001f937\"\n",
        "      u'\\U00010000-\\U0010ffff'\n",
        "      u\"\\u200d\"\n",
        "      u\"\\u2640-\\u2642\"\n",
        "      u\"\\u2600-\\u2B55\"\n",
        "      u\"\\u23cf\"\n",
        "      u\"\\u23e9\"\n",
        "      u\"\\u231a\"\n",
        "      u\"\\u3030\"\n",
        "      u\"\\ufe0f\"\n",
        "      u\"\\u2069\"\n",
        "      u\"\\u2066\"\n",
        "      u\"\\u200c\"\n",
        "      u\"\\u2068\"\n",
        "      u\"\\u2067\"\n",
        "      \"]+\", flags=re.UNICODE)\n",
        "  sent = wierd_pattern.sub(r'', sent)\n",
        "\n",
        "  if(bool(re.match('^[a-zA-Z]',sent))==True):\n",
        "    sent=f2p(sent)\n",
        "\n",
        "  sent = re.sub(\"#\", \"\", sent)\n",
        "  sent = re.sub(\"\\s+\", \" \", sent)\n",
        "\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thc_34Wie51K"
      },
      "outputs": [],
      "source": [
        "# import string\n",
        "# lemmatizer=hazm.Lemmatizer()\n",
        "# def tokenize(sent):\n",
        "#     clean=[lemmatizer.lemmatize(word).split(\"#\")[0] for word in hazm.word_tokenize(sent) if (word not in string.punctuation + \"٬\" + \"،\")]\n",
        "#     return ' '.join(clean)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer=hazm.Normalizer()\n",
        "\n",
        "train_df['comment']= train_df['comment'].apply(lambda x : normalizer.normalize(x))\n",
        "val_df['comment']= val_df['comment'].apply(lambda x : normalizer.normalize(x))\n",
        "test_df['comment']= test_df['comment'].apply(lambda x : normalizer.normalize(x))"
      ],
      "metadata": {
        "id": "ZgTpwvWvsNRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['comment']= train_df['comment'].apply(cleaning)#.apply(tokenize)\n",
        "val_df['comment']= val_df['comment'].apply(cleaning)#.apply(tokenize)\n",
        "test_df['comment']= test_df['comment'].apply(cleaning)#.apply(tokenize)"
      ],
      "metadata": {
        "id": "pIrP6tRzsPRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.astype({'label_id': 'int', 'label': 'str', 'comment': 'str'})\n",
        "val_df = val_df.astype({'label_id': 'int', 'label': 'str', 'comment': 'str'})\n",
        "test_df = test_df.astype({'label_id': 'int', 'label': 'str', 'comment': 'str'})"
      ],
      "metadata": {
        "id": "dPyzwsCfskWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZli2lqee7e1"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[['comment', 'label_id']]\n",
        "val_df = val_df[['comment', 'label_id']]\n",
        "test_df = test_df[['comment', 'label_id']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = train_df['comment'].values.tolist(), train_df['label_id'].values.tolist()\n",
        "x_val, y_val = val_df['comment'].values.tolist(), val_df['label_id'].values.tolist()\n",
        "x_test, y_test = test_df['comment'].values.tolist(), test_df['label_id'].values.tolist()"
      ],
      "metadata": {
        "id": "ghgOLfEGzhSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp3xKmn69unK"
      },
      "source": [
        "## BERT requierment libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNxuxBeJy6Q1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNsqqE6OmONY"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertTokenizer,BertModel,AdamW,get_linear_schedule_with_warmup\n",
        "from transformers import TFBertModel, TFBertForSequenceClassification\n",
        "from transformers import glue_convert_examples_to_features\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km1vDLLRmPnn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import string\n",
        "import json\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukk7YC4m9-nI"
      },
      "source": [
        "## configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udda1_W0rhHU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {device}')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVgIXqzutwS4"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 50\n",
        "\n",
        "\n",
        "\n",
        "EEVERY_EPOCH = 1000\n",
        "\n",
        "CLIP = 0.0\n",
        "\n",
        "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
        "OUTPUT_PATH = '/content/model.bin'\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l08pC_Bz2OB"
      },
      "outputs": [],
      "source": [
        "label2id = {'SAD': 1, 'HAPPY': 0}\n",
        "id2label = {1: 'SAD', 0: 'HAPPY'}\n",
        "\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calling for bert tokenizer"
      ],
      "metadata": {
        "id": "Asxoy5B5aWkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB33oNOB1VXO"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "config = BertConfig.from_pretrained(MODEL_NAME_OR_PATH, **{'label2id': label2id,'id2label': id2label})\n",
        "print(config.to_json_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create class to create embedding vec and masked attention vec and ..."
      ],
      "metadata": {
        "id": "Ei3DWyqOacRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A2lxGHK1bYl"
      },
      "outputs": [],
      "source": [
        "class InputExample:\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "def make_examples(tokenizer, x, y=None, maxlen=50, output_mode=\"classification\", is_tf_dataset=True):\n",
        "    examples = []\n",
        "    y = y if isinstance(y, list) or isinstance(\n",
        "        y, np.ndarray) else [None] * len(x)\n",
        "\n",
        "    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n",
        "        guid = \"%s\" % i\n",
        "        label = int(_y)\n",
        "\n",
        "        if isinstance(_x, str):\n",
        "            text_a = _x\n",
        "            text_b = None\n",
        "        else:\n",
        "            assert len(_x) == 2\n",
        "            text_a = _x[0]\n",
        "            text_b = _x[1]\n",
        "\n",
        "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "\n",
        "    features = glue_convert_examples_to_features(\n",
        "        examples,\n",
        "        tokenizer,\n",
        "        maxlen,\n",
        "        output_mode=output_mode,\n",
        "        label_list=list(np.unique(y)))\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_token_type_ids = []\n",
        "    all_labels = []\n",
        "\n",
        "    for f in tqdm(features, position=0, total=len(examples)):\n",
        "        if is_tf_dataset:\n",
        "            all_input_ids.append(tf.constant(f.input_ids))\n",
        "            all_attention_masks.append(tf.constant(f.attention_mask))\n",
        "            all_token_type_ids.append(tf.constant(f.token_type_ids))\n",
        "            all_labels.append(tf.constant(f.label))\n",
        "        else:\n",
        "            all_input_ids.append(f.input_ids)\n",
        "            all_attention_masks.append(f.attention_mask)\n",
        "            all_token_type_ids.append(f.token_type_ids)\n",
        "            all_labels.append(f.label)\n",
        "\n",
        "    if is_tf_dataset:\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "            'input_ids': all_input_ids,\n",
        "            'attention_mask': all_attention_masks,\n",
        "            'token_type_ids': all_token_type_ids\n",
        "        }, all_labels))\n",
        "\n",
        "        return dataset, features\n",
        "\n",
        "    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n",
        "    ydata = all_labels\n",
        "\n",
        "    return [xdata, ydata], features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_wycaB22IaR"
      },
      "outputs": [],
      "source": [
        "train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=50)\n",
        "valid_dataset_base, valid_examples = make_examples(tokenizer, x_val, y_val, maxlen=50)\n",
        "\n",
        "test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=50)\n",
        "[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=50, is_tf_dataset=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get train val and test sets"
      ],
      "metadata": {
        "id": "6UQrswNVaptX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip89Swjk5LhD"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(dataset, batch_size):\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_validation_dataset(dataset, batch_size):\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "NTi1Pu2L2Bx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3gqc5aa5hVF"
      },
      "outputs": [],
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\n",
        "valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n",
        "\n",
        "train_steps = len(train_examples) // TRAIN_BATCH_SIZE\n",
        "valid_steps = len(valid_examples) // VALID_BATCH_SIZE\n",
        "\n",
        "train_steps, valid_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create model"
      ],
      "metadata": {
        "id": "5fqS7IbGayEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpjm2R7n5t7o"
      },
      "outputs": [],
      "source": [
        "def build_model(model_name, config, learning_rate=.000001):\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        model_name, config=config)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3c8p92p53sU"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = .00001\n",
        "model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## learning"
      ],
      "metadata": {
        "id": "3dGP10Qaa0iX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU4EfqTw55ef"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=valid_steps,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ56Sktf5-6O"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation"
      ],
      "metadata": {
        "id": "ToS1ILIVa4h2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ywIxJyA6DMA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))\n",
        "print(f'\\nEvaluation: {ev}')\n",
        "\n",
        "\n",
        "predictions = model.predict(xtest)\n",
        "ypred = predictions[0].argmax(axis=-1).tolist()\n",
        "\n",
        "print(classification_report(ytest, ypred, target_names=['SAD', 'HAPPY']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], 'bo-', label='Train')\n",
        "plt.plot(history.history['val_loss'], 'ro-', label='Valid')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Loss')"
      ],
      "metadata": {
        "id": "16d9EsxI842j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], 'bo-', label='Train')\n",
        "plt.plot(history.history['val_accuracy'], 'ro-', label='Valid')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('ACCURACY')"
      ],
      "metadata": {
        "id": "BgnmCwzt3pP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExtmjfOTuduz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after you fix the labels of test set:\n",
        "# this indexes get from the pytorch iimplementation and show the index of records that have flase label_id\n",
        "\n",
        "x_test_fix= .values.tolist()\n",
        "y_test_fix= .values.tolist()\n",
        "[xtest_new, ytest_new], test_examples_new = make_examples(tokenizer, x_test_fix, y_test_fix, maxlen=50, is_tf_dataset=False)\n",
        "\n",
        "new_predictions = model.predict(xtest_new)\n",
        "ypred_new = new_predictions[0].argmax(axis=-1).tolist()\n",
        "\n",
        "print(classification_report(ytest_new, ypred_new, target_names=['SAD', 'HAPPY']))"
      ],
      "metadata": {
        "id": "8oFGyc7rw3Yb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}